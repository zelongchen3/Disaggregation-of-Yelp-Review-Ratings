{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_topics, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if topic_idx < num_topics:\n",
    "            print(\"{:11}\".format(\"Topic %d:\" %(topic_idx)), end='')\n",
    "            print(\", \".join(['{:04.3f}*'.format(topic[i])+feature_names[i] \\\n",
    "                             for i in topic.argsort()[:-no_top_words-1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars_x</th>\n",
       "      <th>categories</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>273</td>\n",
       "      <td>6fPQJq4f_yiq1NHn0fd11Q</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89109.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>French, Restaurants, Creperies</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-10-27 17:24:21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>jByDg2ZFV0rtPabku-unxw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This is always a must visit on vegas trips.  I...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6e0khvHCOJU1YTCx8gDfSw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>274</td>\n",
       "      <td>6fPQJq4f_yiq1NHn0fd11Q</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89109.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>French, Restaurants, Creperies</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-08-24 22:41:59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M8ebYJfjl6MkqfMGxywv-A</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AWESOME! Im not even a crepe person.... which ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>m2j1IYqreZKF1crpx5-7Cg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>275</td>\n",
       "      <td>6fPQJq4f_yiq1NHn0fd11Q</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89109.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>French, Restaurants, Creperies</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-02-19 08:00:59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Db3RMWo2sSg22norLqeVGQ</td>\n",
       "      <td>4.0</td>\n",
       "      <td>God selection of crepes! Not much of s wait, r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z2FuxpUUQ1pTbolsCxHPXw</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>276</td>\n",
       "      <td>6fPQJq4f_yiq1NHn0fd11Q</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89109.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>French, Restaurants, Creperies</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-11 16:42:57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>iAd4kyeMsNtmveg86zmzkg</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Decent quality - they're super fast to get you...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rytnXVNx7NJMx6BGz0vsqw</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>277</td>\n",
       "      <td>6fPQJq4f_yiq1NHn0fd11Q</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89109.0</td>\n",
       "      <td>535.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>French, Restaurants, Creperies</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017-01-14 21:43:08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UhUH7iZzNumvgj-UVQGzyA</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Fast, friendly service and a little crepe stan...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Zohjr4ZPl76vryPqiQLJ4A</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1             business_id         name       city  \\\n",
       "0           0           273  6fPQJq4f_yiq1NHn0fd11Q  La Creperie  Las Vegas   \n",
       "1           1           274  6fPQJq4f_yiq1NHn0fd11Q  La Creperie  Las Vegas   \n",
       "2           2           275  6fPQJq4f_yiq1NHn0fd11Q  La Creperie  Las Vegas   \n",
       "3           3           276  6fPQJq4f_yiq1NHn0fd11Q  La Creperie  Las Vegas   \n",
       "4           4           277  6fPQJq4f_yiq1NHn0fd11Q  La Creperie  Las Vegas   \n",
       "\n",
       "  state  postal_code  review_count  stars_x                      categories  \\\n",
       "0    NV      89109.0         535.0      3.5  French, Restaurants, Creperies   \n",
       "1    NV      89109.0         535.0      3.5  French, Restaurants, Creperies   \n",
       "2    NV      89109.0         535.0      3.5  French, Restaurants, Creperies   \n",
       "3    NV      89109.0         535.0      3.5  French, Restaurants, Creperies   \n",
       "4    NV      89109.0         535.0      3.5  French, Restaurants, Creperies   \n",
       "\n",
       "   cool                 date  funny               review_id  stars_y  \\\n",
       "0   0.0  2011-10-27 17:24:21    0.0  jByDg2ZFV0rtPabku-unxw      4.0   \n",
       "1   0.0  2011-08-24 22:41:59    0.0  M8ebYJfjl6MkqfMGxywv-A      4.0   \n",
       "2   0.0  2017-02-19 08:00:59    0.0  Db3RMWo2sSg22norLqeVGQ      4.0   \n",
       "3   0.0  2016-04-11 16:42:57    0.0  iAd4kyeMsNtmveg86zmzkg      4.0   \n",
       "4   0.0  2017-01-14 21:43:08    0.0  UhUH7iZzNumvgj-UVQGzyA      4.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  This is always a must visit on vegas trips.  I...     0.0   \n",
       "1  AWESOME! Im not even a crepe person.... which ...     1.0   \n",
       "2  God selection of crepes! Not much of s wait, r...     0.0   \n",
       "3  Decent quality - they're super fast to get you...     0.0   \n",
       "4  Fast, friendly service and a little crepe stan...     2.0   \n",
       "\n",
       "                  user_id  userid  \n",
       "0  6e0khvHCOJU1YTCx8gDfSw       0  \n",
       "1  m2j1IYqreZKF1crpx5-7Cg       1  \n",
       "2  Z2FuxpUUQ1pTbolsCxHPXw       2  \n",
       "3  rytnXVNx7NJMx6BGz0vsqw       3  \n",
       "4  Zohjr4ZPl76vryPqiQLJ4A       4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in Data\n",
    "data = pd.read_csv('data_89109.csv')\n",
    "data['userid'] = data['Unnamed: 0']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414764"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>name</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>text</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>273</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>4.0</td>\n",
       "      <td>This is always a must visit on vegas trips.  I...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AWESOME! Im not even a crepe person.... which ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>275</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>4.0</td>\n",
       "      <td>God selection of crepes! Not much of s wait, r...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Decent quality - they're super fast to get you...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277</td>\n",
       "      <td>La Creperie</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Fast, friendly service and a little crepe stan...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1         name  stars_y  \\\n",
       "0           273  La Creperie      4.0   \n",
       "1           274  La Creperie      4.0   \n",
       "2           275  La Creperie      4.0   \n",
       "3           276  La Creperie      4.0   \n",
       "4           277  La Creperie      4.0   \n",
       "\n",
       "                                                text  userid  \n",
       "0  This is always a must visit on vegas trips.  I...       0  \n",
       "1  AWESOME! Im not even a crepe person.... which ...       1  \n",
       "2  God selection of crepes! Not much of s wait, r...       2  \n",
       "3  Decent quality - they're super fast to get you...       3  \n",
       "4  Fast, friendly service and a little crepe stan...       4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['Unnamed: 0','categories','date','stars_x','cool','review_id','funny','business_id','city','state','postal_code','review_count','useful','user_id'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split reviews into individual sentences \n",
    "df = pd.DataFrame(columns=['userid','sentence','stars'])\n",
    "for i in range(0,60000,1):\n",
    "    sentences = tokenize.sent_tokenize(data.text[i])\n",
    "    for j in sentences:\n",
    "        df = df.append({'userid':data.userid[i],'sentence':j,'stars':data.stars_y[i]},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus for TFIDF choose the first 60000 rows\n",
    "corpus_3 = []\n",
    "for i in df.sentence:\n",
    "        corpus_3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df.to_csv('89109_1.csv')\n",
    "#corpus_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "df.to_csv('89109_2.csv')\n",
    "#corpus_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "df.to_csv('89109_3.csv')\n",
    "#corpus_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_csv('89109_3.csv')\n",
    "corpus_3 = []\n",
    "for i in df_3.sentence:\n",
    "        corpus_3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Corpus for TFIDF\n",
    "corpus = []\n",
    "corpus = corpus_1+corpus_2+corpus_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: great experience atmosphere overall staff drinks time meal selection location server restaurant spot ambiance breakfast\n",
      "Topic #1: good pretty really pizza overall experience drinks selection thing buffet sushi price fries desserts flavor\n",
      "Topic #2: vegas best las buffet time ve favorite trip visit strip restaurant buffets hotel eat bellagio\n",
      "Topic #3: food quality ok decent excellent price average better awesome mediocre selection drinks portions chinese okay\n",
      "Topic #4: service excellent friendly customer slow attentive fast quick horrible staff terrible stars bad awesome nice\n",
      "Topic #5: place love recommend try eat awesome loved fun stars highly nice looking strip cool overall\n",
      "Topic #6: just really like time wait don didn got came nice ordered chicken order pizza went\n",
      "Topic #7: amazing chicken absolutely experience pizza fried cheese burger staff server mac simply waffles sushi ordered\n",
      "Topic #8: delicious pizza chicken absolutely fresh ordered cheese fried burger salad fries sandwich sauce mac best\n",
      "Topic #9: definitely come recommend worth try highly return ll coming time wait price visit going money\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_components = 10\n",
    "n_top_words = 15\n",
    "\n",
    "# TFIDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# NMF reduction\n",
    "nmf = NMF(n_components=n_components).fit(tfidf)\n",
    "W_pos = nmf.fit_transform(tfidf)\n",
    "\n",
    "# Output Topics\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: great experience atmosphere overall time staff drinks meal selection location server restaurant ambiance spot breakfast\n",
      "Topic #1: good pretty really pizza overall experience drinks selection buffet thing sushi price desserts prices fries\n",
      "Topic #2: vegas definitely time best come las buffet ve favorite visit trip try recommend restaurant eat\n",
      "Topic #3: food amazing quality delicious ok decent excellent average price better mediocre drinks awesome selection portions\n",
      "Topic #4: service excellent friendly customer amazing slow attentive fast quick staff horrible terrible stars bad awesome\n",
      "Topic #5: place love recommend amazing try highly eat loved fun awesome stars nice looking definitely overall\n",
      "Topic #6: just really like delicious wait chicken got don didn pizza ordered nice worth came order\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_components = 7\n",
    "n_top_words = 15\n",
    "\n",
    "# TFIDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# NMF reduction\n",
    "nmf = NMF(n_components=n_components).fit(tfidf)\n",
    "W_pos = nmf.fit_transform(tfidf)\n",
    "\n",
    "# Output Topics\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic #0: Overall experience\n",
    "- Topic #1: Price\n",
    "- Topic #2: \"Worth it\"\n",
    "- Topic #3: Food\n",
    "- Topic #4: Service\n",
    "- Topic #5: \n",
    "- Topic #6: Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path =r'C:\\Users\\xinro\\Downloads\\89109'\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_)\n",
    "df = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Topic with highest score\n",
    "array = []\n",
    "# For all NMF array\n",
    "for i in range(0,len(W_pos),1):\n",
    "    # Create dictionary with Topics and its NMF scores for each sentence\n",
    "    topic_dict = {}\n",
    "    # Drop sentences that have length less than 10 by setting topic to -1\n",
    "    if len(corpus[i])>=10:\n",
    "        for ind, w in enumerate(W_pos[i]):\n",
    "            topic_dict[ind] = w\n",
    "        # Classify sentence to the topic with highest score\n",
    "        array.append(max(topic_dict, key=topic_dict.get))\n",
    "    else:\n",
    "        array.append(-1)\n",
    "# Create new column in df for topic\n",
    "df['Topic'] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Sentiment Intensity Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Sentiment Intensity Scores for each sentence\n",
    "array = []\n",
    "for i in df.sentence:\n",
    "    # Generate Sentiment Intensity Scores and store in array\n",
    "    score = analyser.polarity_scores(i)\n",
    "    array.append(score['compound'])\n",
    "# Create new column in df for sentiment intensity score\n",
    "df['sentiment'] = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Final df of intensity scores\n",
    "df_scores = pd.DataFrame(columns=['userid','0','1','2','3','4','5','6','stars'])\n",
    "# For every user aggregate the sentiment scores by topic\n",
    "for i in df.userid.unique():\n",
    "    # Create df of scores from same user\n",
    "    temp_df = df[df.userid==i].reset_index(drop=True)\n",
    "    # For every topic\n",
    "    topic_score = []\n",
    "    for j in range(0,7,1):\n",
    "        score = 0\n",
    "        count = 0\n",
    "        for k in range(0,len(temp_df),1):\n",
    "            # If topic equal to current topic\n",
    "            if temp_df.Topic[k] == j:\n",
    "                # Add sentiment score\n",
    "                score = score + temp_df.sentiment[k]\n",
    "                # Increase count\n",
    "                count = count + 1\n",
    "        # If count = 0 then no score for topic\n",
    "        if count==0:\n",
    "            topic_score.append(0)\n",
    "        # Else append average score for topic\n",
    "        else:\n",
    "            topic_score.append(score/count)\n",
    "    # Insert UserId and Star Rating \n",
    "    topic_score.insert(0,temp_df.userid[0])\n",
    "    topic_score.insert(len(topic_score),temp_df.stars[0])\n",
    "    # Transform and Append into main df\n",
    "    temp = pd.DataFrame(pd.Series(topic_score))\n",
    "    temp = temp.transpose()\n",
    "    temp.columns = df_scores.columns\n",
    "    df_scores = df_scores.append(temp,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into predictors and target\n",
    "X = df_scores.drop(['userid','stars'],axis=1)\n",
    "X = X.astype(float)\n",
    "y = df_scores.stars\n",
    "y = y.astype(float)\n",
    "# Split Train vs Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=1)\n",
    "# Split Test set into validation & test set\n",
    "X_test2, X_val, y_test2, y_val = train_test_split(X_test,y_test,test_size=0.5,stratify=y_test,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const    2.871471\n",
      "0        0.628335\n",
      "1        0.315986\n",
      "2        0.850981\n",
      "3        0.838317\n",
      "4        0.896340\n",
      "5        0.753629\n",
      "6        1.653815\n",
      "dtype: float64\n",
      "\n",
      "Mean Squared Error:  1.2035580251310127\n",
      "AIC:  144970.80224183755\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y_train,sm.add_constant(X_train)).fit()\n",
    "print(model.params)\n",
    "print()\n",
    "print('Mean Squared Error: ',mean_squared_error(y_val,model.predict(sm.add_constant(X_val))))\n",
    "print('AIC: ',model.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Topic #0: Overall experience\n",
    "- Topic #1: Price\n",
    "- Topic #2: \"Worth it\"\n",
    "- Topic #3: Food\n",
    "- Topic #4: Service\n",
    "- Topic #5: \n",
    "- Topic #6: Food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ùë¶‚Ñéùëéùë°=2.87+0.63‚àóOverall+0.32‚àóPrice+0.85‚àóùëäùëúùëüùë°‚Ñé+0.84‚àóùêπùëúùëúùëë+0.90‚àóService+0.75Topic5+1.65‚àóFood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2.163695\n",
      "1    2.749672\n",
      "2    2.628318\n",
      "3    2.197884\n",
      "5    2.299617\n",
      "6    4.525110\n",
      "dtype: float64\n",
      "\n",
      "Mean Squared Error:  5.030062303354737\n",
      "AIC:  213708.88908207347\n"
     ]
    }
   ],
   "source": [
    "# Split into predictors and target\n",
    "X = df_scores.drop(['userid','stars','4'],axis=1)\n",
    "X = X.astype(float)\n",
    "y = df_scores.stars\n",
    "y = y.astype(float)\n",
    "# Split Train vs Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=1)\n",
    "# Split Test set into validation & test set\n",
    "X_test2, X_val, y_test2, y_val = train_test_split(X_test,y_test,test_size=0.5,stratify=y_test,random_state=1)\n",
    "\n",
    "model = sm.OLS(y_train,X_train).fit()\n",
    "print(model.params)\n",
    "print()\n",
    "print('Mean Squared Error: ',mean_squared_error(y_val,model.predict(X_val)))\n",
    "print('AIC: ',model.aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
